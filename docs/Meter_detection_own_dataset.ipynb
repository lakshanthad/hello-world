{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meter-detection-own-dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7z3fSlmB6CZIlYf6AAuXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshanthad/hello-world/blob/master/docs/Meter_detection_own_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will guide you to train an analog meter reading detection model with your own dataset!"
      ],
      "metadata": {
        "id": "4wCnaloE6aew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configure host environment"
      ],
      "metadata": {
        "id": "tiVpbLJRMvZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Choose **GPU** in **Runtime** if not already selected by navigating to `Runtime --> Change Runtime Type --> Hardware accelerator --> GPU`"
      ],
      "metadata": {
        "id": "6VrcRq-U6G2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.** Clone Edgelab repo and access it"
      ],
      "metadata": {
        "id": "K8prsdf8u8Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~\n",
        "!git clone https://github.com/Seeed-Studio/Edgelab\n",
        "%cd Edgelab"
      ],
      "metadata": {
        "id": "ZfnLgOjgu8ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.** Configure the environment by running the following script which will download and install the relevant dependencies"
      ],
      "metadata": {
        "id": "xPagIdHSaY6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/env_config.py"
      ],
      "metadata": {
        "id": "C32Eow5zafe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4.** After the relevant environment configuration is completed, the path of this project needs to be configured in the PYTHONPATH environment variable. Configure it as follows"
      ],
      "metadata": {
        "id": "FN2ixxSEE2K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = os.environ['HOME']\n",
        "os.environ['PYTHONPATH'] = f'{home}/Edgelab'"
      ],
      "metadata": {
        "id": "hiyU2Po6LeWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate firmware image\n",
        "\n",
        "We need to generate a firmware image for the Grove - Vision AI to support the analog meter reading detection model because the default firmware which is pre-installed out-of-the-box does not support it."
      ],
      "metadata": {
        "id": "_FmAXRZBDm1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to download an already pre-compiled firmware, please follow the links below. But we recommend you to compile the firmware from source, so that you will have the latest firmware always.\n",
        "\n",
        "- [Grove - Vision AI Module](https://files.seeedstudio.com/wiki/Edgelab/uf2/grove-vision-ai-firmware.uf2)\n",
        "- [SenseCAP A1101](https://files.seeedstudio.com/wiki/Edgelab/uf2/sensecap-A1101-firmware.uf2)"
      ],
      "metadata": {
        "id": "LY_3XyBXFdZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Download GNU Development Toolkit"
      ],
      "metadata": {
        "id": "SQoLqgXxFkeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~\n",
        "!wget https://github.com/foss-for-synopsys-dwc-arc-processors/toolchain/releases/download/arc-2020.09-release/arc_gnu_2020.09_prebuilt_elf32_le_linux_install.tar.gz"
      ],
      "metadata": {
        "id": "0ei9sSIfFpWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.** Extract the file"
      ],
      "metadata": {
        "id": "U9neMfXoFvhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf arc_gnu_2020.09_prebuilt_elf32_le_linux_install.tar.gz"
      ],
      "metadata": {
        "id": "7iA1fOR7F30y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Add **arc_gnu_2020.09_prebuilt_elf32_le_linux_install/bin** to **PATH**"
      ],
      "metadata": {
        "id": "yty9F8gGF-wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PATH'] += f':{home}/arc_gnu_2020.09_prebuilt_elf32_le_linux_install/bin'"
      ],
      "metadata": {
        "id": "eXx8WlTVGCow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Execute the following to install required dependencies "
      ],
      "metadata": {
        "id": "-T43ThLFaUKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "!wget -4c https://ftp.gnu.org/gnu/glibc/glibc-2.29.tar.gz\n",
        "!tar -zxvf glibc-2.29.tar.gz\n",
        "!apt-get install gawk bison -y\n",
        "%cd glibc-2.29 \n",
        "%mkdir build\n",
        "%cd build\n",
        "!../configure --prefix=/usr/local --disable-sanity-checks\n",
        "!make -j8\n",
        "!make install\n",
        "%cd /lib/x86_64-linux-gnu\n",
        "!cp /usr/local/lib/libm-2.29.so /lib/x86_64-linux-gnu/\n",
        "!ln -sf libm-2.29.so libm.so.6"
      ],
      "metadata": {
        "id": "_B2Ui021aWjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:** Navigate to the following repo of Edgelab"
      ],
      "metadata": {
        "id": "_fAvHImHSdiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/Edgelab/examples/vision_ai"
      ],
      "metadata": {
        "id": "TgiH10YjLppz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6:** Download related tools\n"
      ],
      "metadata": {
        "id": "PMlFGZSbTR3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make download"
      ],
      "metadata": {
        "id": "MP4F6ULjTS-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7:** Compile the firmware according to your hardware "
      ],
      "metadata": {
        "id": "5TJ7sTcTTYm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For Grove - Vision AI Module"
      ],
      "metadata": {
        "id": "GckkHv9GTb5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make HW=grove_vision_ai APP=meter\n",
        "!make flash"
      ],
      "metadata": {
        "id": "kKeUCBVHTnbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For SenseCAP A1101"
      ],
      "metadata": {
        "id": "psGhUcVIY2qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make HW=sensecap_vision_ai APP=meter\n",
        "!make flash"
      ],
      "metadata": {
        "id": "Yug49NEGY6oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above will generate output.img inside **~/Edgelab/examples/vision_ai/tools/image_gen_cstm/output/** directory"
      ],
      "metadata": {
        "id": "1paJ3XZXZnZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step *:** Generate firmware image **firmware.uf2** file so that we can later flash it directly to the Grove - Vision AI Module/ SenseCAP A1101"
      ],
      "metadata": {
        "id": "eJWaZ_KtZ54S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tools/ufconv/uf2conv.py -t 0 -c tools/image_gen_cstm/output/output.img -o firmware.uf2"
      ],
      "metadata": {
        "id": "IZ6nY4pQaG_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate **firmware.uf2** inside **~/Edgelab/examples/vision_ai** directory\n",
        "\n"
      ],
      "metadata": {
        "id": "QFLhPf3Hac7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8:** Download the **firmware.uf2** file"
      ],
      "metadata": {
        "id": "Wd2znoeFbJON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"firmware.uf2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4F0rVRXhahV0",
        "outputId": "3fb00481-423d-4cc2-d6db-600eedf91db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6e6ee858-4d3b-40f7-a336-9f84cfc78373\", \"firmware.uf2\", 901632)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare dataset"
      ],
      "metadata": {
        "id": "iebJSPm-NHfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we explain how to use your own dataset\n",
        "\n",
        "### Collect dataset\n",
        "\n",
        "If you want to train your own analog meter detection model for a specific application, you need to spend sometime to collect images to prepare a dataset. Here you can take several photos (start with 200 and go higher to improve accuracy) of the analog meter that you want to detect with the meter pointer at different points and also take photos at different lighting conditions and different environments as follows\n",
        "\n",
        "#### Pointer reading at dark environment \n",
        "\n",
        "<div align=center><img width=350 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/9.jpg\"/></div>\n",
        "\n",
        "#### Pointer reading at light environment \n",
        "\n",
        "<div align=center><img width=350 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/10.jpg\"/></div>\n",
        "\n",
        "### Annotate dataset\n",
        "\n",
        "Next we need to annotate all the images that we have collected. Here we will use an application called **labelme** which is an open source image annotation tool.\n",
        "\n",
        "- **Step 1.** Visit [this page](https://github.com/wkentaro/labelme#installation) and install labelme according to your operating system\n",
        "\n",
        "- **Step 2.** On the command-line, type the following to open **labelme**\n",
        "\n",
        "```sh\n",
        "labelme\n",
        "```\n",
        "\n",
        "- **Step 3.** Once labelme opens, click on **OpenDir**, select the folder that you have put all the collected images and click **Select Folder**\n",
        "\n",
        "<div align=center><img width=350 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/1.jpg\"/></div>\n",
        "\n",
        "<div align=center><img width=550 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/2.png\"/></div>\n",
        "\n",
        "- **Step 4.** Later, when we annotate images, labelme will generate a **json file** for each image and this file will contain the annotation information for the corresponsing image. Here we need to specify a directory to store these image annotations because we recommend to store these json files and image files in 2 different folders. Go to `File > Change Output Dir`\n",
        "\n",
        "<div align=center><img width=250 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/3.jpg\"/></div>\n",
        "\n",
        "- **Step 5.** Create a new folder, select the folder and click **Select Folder** \n",
        "\n",
        "<div align=center><img width=550 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/4.jpg\"/></div>\n",
        "\n",
        "- **Step 6.** Go to `File > Save Automatically` to save time when annotating all the images. Otherwise it will pop up a prompt to save each image.\n",
        "\n",
        "<div align=center><img width=250 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/5.png\"/></div>\n",
        "\n",
        "- **Step 7.** Right click on the first opened image and select **Create Point**\n",
        "\n",
        "<div align=center><img width=550 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/6.jpg\"/></div>\n",
        "\n",
        "- **Step 8.** Draw a point at the tip of the pointer, set any label name and click **OK**\n",
        "\n",
        "<div align=center><img width=350 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/25.png\"/></div>\n",
        "\n",
        "<div align=center><img width=550 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/26.png\"/></div>\n",
        "\n",
        "After the point, there will be a new **json file** created automatically for each image file under \"annotations\" folder as mentioned before.\n",
        "\n",
        "### Organize dataset\n",
        "\n",
        "Now you need to manually organize the dataset by splitting all the images and annotations into **train, val, test** directories as follows\n",
        "\n",
        "Here we recommend you to split in the following percentages\n",
        "\n",
        "- train = 80%\n",
        "- val = 10%\n",
        "- test = 10%\n",
        "\n",
        "So for example, if you have 200 images, the split would be\n",
        "\n",
        "- train = 160 images\n",
        "- val = 20 images\n",
        "- test = 20 images\n",
        "\n",
        "```\n",
        "meter_data\n",
        "    |train\n",
        "        |images\n",
        "            |a.jpg\n",
        "            |b.jpg\n",
        "        |annotations\n",
        "            |a.json\n",
        "            |b.json\n",
        "    |val\n",
        "        |images\n",
        "            |c.jpg\n",
        "            |d.jpg\n",
        "        |annotations\n",
        "            |c.json\n",
        "            |d.json\n",
        "    |test\n",
        "        |images\n",
        "            |e.jpg\n",
        "            |f.jpg\n",
        "        |annotations\n",
        "            |e.json\n",
        "            |f.json\n",
        "```"
      ],
      "metadata": {
        "id": "FUs90xSecXSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Start training"
      ],
      "metadata": {
        "id": "CeUCh_ocdV5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration file \n",
        "\n",
        "We will choose the profile according to the task that we want to implement. We have prepared preconfigured files inside the the [configs](https://github.com/Seeed-Studio/edgelab/tree/master/configs) folder.\n",
        "\n",
        "For our meter reading detection example, we will use [pfld_mv2n_112.py](https://github.com/Seeed-Studio/Edgelab/blob/master/configs/pfld/pfld_mv2n_112.py) config file. This file will be mainly used to configure the dataset for training including the dataset location."
      ],
      "metadata": {
        "id": "sZ030qVodtUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can use your own dataset for training.\n",
        "Execute the following command inside the activated conda virtual environment terminal to start training an end-to-end analog meter reading detection model."
      ],
      "metadata": {
        "id": "Eq1qGpKleE0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!~/anaconda3/envs/edgelab/bin/python tools/train.py mmpose configs/pfld/pfld_mv2n_112.py --gpus=1 --cfg-options runner.max_epochs=100 data.train.index_file=/meter/train/annotations data.train.img_dir=/meter/train/images data.val.index_file=/meter/val/annotations data.val.img_dir=/meter/val/images data.test.index_file=/meter/test/annotations data.test.img_dir=/meter/test/images"
      ],
      "metadata": {
        "id": "jDIZh8Y_dXqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Make sure to change the **img_dir** and **index_file** locations according to your dataset images and annotations path"
      ],
      "metadata": {
        "id": "GzL5CBarpsf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format of the above command looks like below\n",
        "\n",
        "```sh\n",
        "python tools/train.py <task_type> <config_file_location> --gpus=<cpu_or_gpu> --cfg-options runner.max_epochs=<number_of_epochs> data.train.index_file=<absolute_path_to_annotations_in_train> data.train.img_dir=<absolute_path_to_images_in_train> data.val.index_file=<absolute_path_to_annotations_in_val> data.val.img_dir=<absolute_path_to_images_in_val> data.test.index_file=<absolute_path_to_annotations_in_test> data.test.img_dir=<absolute_path_to_images_in_test>\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "- <task_type> refers to either **mmcls** for classfication, **mmdet** for detection and **mmpose** for pose estimation\n",
        "- <config_file_location> refers to the path where the model configuration is located \n",
        "- <cpu_or_gpu> refers to specifying whether you want to train on CPU or GPU. Type **0** CPU and **1** for GPU\n",
        "- --cfg-options runner.max_epochs=<number_of_epochs> refers to the number of training cycles\n",
        "- --cfg-options data.train.index_file=<absolute_path_to_annotations_in_train> refers to the location of the annotations files under train set\n",
        "- --cfg-options data.train.img_dir=<absolute_path_to_images_in_train> refers to the location of the images files under train set\n",
        "- --cfg-options data.val.index_file=<absolute_path_to_annotations_in_val> refers to the location of the annotations files under validation set\n",
        "- --cfg-options data.val.index_file=<absolute_path_to_annotations_in_val> refers to the location of the image files under validation set\n",
        "- --cfg-options data.test.index_file=<absolute_path_to_annotations_in_test> refers to the location of the annotations files under test set\n",
        "- --cfg-options data.test.img_dir=<absolute_path_to_images_in_test> refers to the location of the image files under test set"
      ],
      "metadata": {
        "id": "9BFh45xWeMu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Export PyTorch to TFLite"
      ],
      "metadata": {
        "id": "XDTRTOjqe74q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the model training is completed, you can export the **.pth** file to the **TFLite** file format. This is important because TFLite format is more optimized to run on low power hardware. Assuming that the environment is in this project path, you can export the models you have trained before to the TFLite format by running the following command:\n",
        "\n"
      ],
      "metadata": {
        "id": "lpPtbM8Be_N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/export.py configs/pfld/pfld_mv2n_112.py --weights work_dirs/pfld_mv2n_112/exp1/latest.pth --data ~/datasets/meter/train/images"
      ],
      "metadata": {
        "id": "5HCCaDQAfFV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate a **latest_int8.tflite** file inside **~/Edgelab/work_dirs/pfld_mv2n_112/exp1** directory"
      ],
      "metadata": {
        "id": "aAneExFVfH_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format of the above command looks like below\n",
        "\n",
        "```sh\n",
        "python tools/export.py configs/xxx/xxx.py --weights <location_to_pth_from_training> --data <location_to_images_directory_of__train_or_val>\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "- configs/xxx/xxx.py refers to the location of the configuration file correcsponsing to the AI model\n",
        "- --weights refers to the the .pth file that was generated during training\n",
        "- --data refers to the images directory of either train or val"
      ],
      "metadata": {
        "id": "CFYKOP9KfUkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Convert TFLite to UF2"
      ],
      "metadata": {
        "id": "Ehr7I4MOfiWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will convert the generated TFLite file to a UF2 file so that we can directly flash the UF2 file into Grove - Vision AI Module and SenseCAP A1101\n",
        "\n"
      ],
      "metadata": {
        "id": "pCFlwF-rfk6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Execute the following"
      ],
      "metadata": {
        "id": "DIZmKayabiSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Edgelab/examples/vision_ai/tools/ufconv/uf2conv.py -f GROVEAI -t 1 -c ~/Edgelab/work_dirs/pfld_mv2n_112/exp1/latest_int8.tflite -o model.uf2"
      ],
      "metadata": {
        "id": "5P_5vU-zfjwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate a **model.uf2** file inside **~/Edgelab/examples/vision_ai** directory\n",
        "\n"
      ],
      "metadata": {
        "id": "tSqePhFJfpCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you only change the location of the TFLite model such as **~/Edgelab/work_dirs/pfld_mv2n_112/exp1/latest_int8.tflite**\n",
        "\n"
      ],
      "metadata": {
        "id": "OPEcHbptftc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Download the uf2 model file"
      ],
      "metadata": {
        "id": "AYQmErmVbkfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"model.uf2\")"
      ],
      "metadata": {
        "id": "Zv2kY5APbmHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Deploy and inference"
      ],
      "metadata": {
        "id": "c0ZkP3MEfyJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flash firmware and model\n",
        "\n",
        "This explains how you can flash the previously generated firmware (firmware.uf2) and the model file (model.uf2) to Grove - Vision AI Module and SenseCAP A1101.\n",
        "\n",
        "**Step 1.** Connect Grove - Vision AI Module/ SenseCAP A1101 to PC by using USB Type-C cable \n",
        "\n",
        "<div align=center><img width=1000 src=\"https://files.seeedstudio.com/wiki/SenseCAP-A1101/45.png\"/></div>\n",
        "\n",
        "**Step 2.** Double click the boot button to enter **boot mode**\n",
        "\n",
        "<div align=center><img width=1000 src=\"https://files.seeedstudio.com/wiki/SenseCAP-A1101/46.png\"/></div>\n",
        "\n",
        "**Step 3:** After this you will see a new storage drive shown on your file explorer as **GROVEAI** for **Grove - Vision AI Module** and as **VISIONAI** for **SenseCAP A1101**\n",
        "\n",
        "<div align=center><img width=500 src=\"https://files.seeedstudio.com/wiki/SenseCAP-A1101/62.jpg\"/></div>\n",
        "\n",
        "**Step 4:** Drag and drop the previous **firmware.uf2** at first, and then the **model.uf2** file to **GROVEAI** or **VISIONAI** \n",
        "\n",
        "Once the copying is finished **GROVEAI** or **VISIONAI** drive will disapper. This is how we can check whether the copying is successful or not."
      ],
      "metadata": {
        "id": "bt_TqW_Bf0Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. View live detection results"
      ],
      "metadata": {
        "id": "JF0JrKhR73B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** After loading the firmware and connecting to PC, visit [this URL](https://files.seeedstudio.com/grove_ai_vision/index.html)\n",
        "\n",
        "**Step 2:** Click **Connect** button. Then you will see a pop up on the browser. Select **Grove AI - Paired** and click **Connect**\n",
        "  \n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/13.jpg\"/></div>\n",
        "\n",
        "<div align=center><img width=400 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/12.png\"/></div>\n",
        "\n",
        "Upon successful connection, you will see a live preview from the camera. Here the camera is pointed at an analog meter.\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/14.png\"/></div>\n",
        "\n",
        "Now we need to set 3 points which is the center point, start point and the end point. \n",
        "\n",
        "**Step 3:** Click on **Set Center Point** and click on the center of the meter. you will see a pop up confirm the location and press **OK**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/15.png\"/></div>\n",
        "\n",
        "You will see the center point is already recorded\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/16.png\"/></div>\n",
        "\n",
        "**Step 4:** Click on **Set Start Point** and click on the first indicator point. you will see a pop up confirm the location and press **OK**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/17.png\"/></div>\n",
        "\n",
        "You will see the first indicator point is already recorded\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/18.png\"/></div>\n",
        "\n",
        "**Step 5:** Click on **Set End Point** and click on the last indicator point. you will see a pop up confirm the location and press **OK**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/19.png\"/></div>\n",
        "\n",
        "You will see the last indicator point is already recorded\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/20.png\"/></div>\n",
        "\n",
        "**Step 6:** Set the measuring range according to the first digit and last digit of the meter. For example, he we set as **From:0 To 0.16**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/21.png\"/></div>\n",
        "\n",
        "**Step 7:** Set the number of decimal places that you want the result to display. Here we set as 2\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/22.png\"/></div>\n",
        "\n",
        "Finally you can see the live meter reading results as follows\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/meter.gif\"/></div>"
      ],
      "metadata": {
        "id": "Pl522ua675uR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ny2ujt9k76ES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}